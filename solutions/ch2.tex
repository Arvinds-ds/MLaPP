
\documentclass{article}
\usepackage{amsmath,amsthm}
\usepackage{cancel}
\usepackage{indentfirst}
\usepackage{commath}
\begin{document}

 % From Wikipedia, not sure where to define this yet...
\def\ci{\perp\!\!\!\perp}
\def\defeq{\buildrel\triangle\over =}
\def\E{\mathrm{E}}
\def\Var{\mathrm{Var}}
\def\Cov{\mathrm{Cov}}

\section{Probabilities are sensitive to the form of the question that
  was used to generate the answer}

\subsection{}

Possible outcomes for $X$, the genders of the two children, are

\[
X\in \left\{ (B,B), (B,G), (G,B), (\cancel{G,G})\right\}
\]

We have eliminated the possiblity of two girls. There are two
remaining outcomes that include at least one girl, so the probability is $\frac{2}{3}$.

\subsection{}

$X \in \{ B, G \}$, so the probability is simply $\frac{1}{2}$.

\section{Legal reasoning}

\subsection{}

\begin{list}{}{}
\item Let $G$ = defendant is guilty
\item Let $B$ = defendant's blood type matches blood type at the scene
\end{list}

The probability that the defendant is guilty given that the
defendant's blood type was found at the scene is then

\[ P(G|B) = \frac{P(G \cap B)}{P(B)} \]

The prosecutor is confusing $P(B)$ for $P(G|B)$. The prosecutor is failing
to account for $P(G)$, the prior probability that the defendant
committed the crime, which is extremely low.

\subsection{}

The defense attorney is ignoring all of the other evidence against the
defendant. Not all of the 8000 people with the matching blood type are
equally likely to have committed the crime.

\section{Variance of a sum}
Prove: $var[X + Y] = var[X] + var[Y] + 2cov[X,Y]$

\begin{align}
  var[X] &= E[X^2]-\mu^2 \\
  cov[X,Y] &= E[XY] - E[X]E[Y]
\end{align}
\begin{align*}
  var[X+Y] &= E[(X+Y)(X+Y)] - E^2[X+Y] \\
           &= E[X^2+2XY+Y^2] - E^2[X+Y] \\
           &= E[X^2+2XY+Y^2] - (\mu_x + \mu_y)^2 \\
           &= E[X^2+2XY+Y^2] - (\mu_x^2 + 2\mu_x\mu_y + \mu_y^2) \\
           &= E[X^2+2XY+Y^2] - \mu_x^2 - 2\mu_x\mu_y - \mu_y^2 \\
           &= E[X^2] - \mu_x^2 + E[Y^2] - \mu_y^2 + 2(E[XY]-\mu_x\mu_y)
             \text{\qquad(By linearity of expectations)}\\
           &= var[X] + var[Y] + 2cov[X,Y] \\
           &= var[X + Y] \qed
\end{align*}

\section{Bayes rule for medical diagnosis}

\begin{list}{}{}
\item Let D = You have the disease
\item Let T = You test positive
\end{list}

\begin{align*}
  P(D) &= \frac{1}{10000} & P(\bar{D}) &= \frac{9999}{10000} \\
  P(T|D) &= \frac{99}{100} & P(T|\bar{D}) &= \frac{1}{100} \\
  P(D|T) &= \frac{P(D \cap T)}{P(T)} = \frac{P(T \cap D)}{P(T)} \\
  P(D|T) &= \frac{P(T|D)P(D)}{P(T)} \\
  P(D|T) &= \frac{P(T|D)P(D)}{P(T|D)P(D) + P(T|\bar{D})P(\bar{D})} \\
  P(D|T) &= \frac{(.99)(.0001)}{(.99)(.0001) + (.01)(.9999)} \\
  P(D|T) &= .98\%
\end{align*}

\section{The Monty Hall problem}

\begin{list}{}{}
\item Let $A$ = the prize is behind door 1
\item Let $B$ = the prize is behind door 2
\item Let $C$ = the prize is behind door 3
\item Let $c$ = the host opens door 3
\end{list}

\begin{align*}
  P(A) = P(B) = P(C) &= \frac{1}{3} \\
  P(c|A) &= \frac{1}{2} \\
  P(c|B) &= 1 \\
  P(c|C) &= 0
\end{align*}

\begin{align*}
  P(B|c) &= \frac{P(c|B)P(B)}{P(c)} \\
  &= \frac{P(c|B)P(B)}{P(c|A)P(A) + P(c|B)P(B) + P(c|C)p(C)} \\
  &= \frac{\frac{1}{3}}{\frac{1}{6} + \frac{2}{6} + 0} \\
  &= \frac{2}{3}
\end{align*}

There is a $\frac{2}{3}$ chance that the prize is behind door 2, so
the contestant should definitely switch.

\section{Conditional independence}
\subsection{}

\begin{align*}
  P(H=k|e_1,e_2) &= \frac{P(H=k \cap e_1,e_2)}{P(e_1,e_2)} \\
  &= \frac{P(e_1 \cap e_2|H=k)P(H=k)}{P(e_1,e_2)} \qquad \text{(ii.)}
\end{align*}

(ii.) is sufficient.

\subsection{}

\begin{equation*}
  E_1 \ci E_2 | H \implies P(e_1,e_2|H) = P(e_1|H) \cdot P(e_2|H)
\end{equation*}
\begin{align*}
    P(H=k|e_1,e_2) &= \frac{P(e_1 \cap e_2|H=k)P(H=k)}{P(e_1,e_2)} \\
    &= \frac{P(e_1|H=k)P(e_2|H=k)P(H=k)}{P(e_1,e_2)} \qquad
    \text{(i.)} \\
    &= \frac{P(e_1|H=k)P(e_2|H=k)P(H=k)}{\sum\limits_{j=1}^K
      P(e_1,e_2|H=j)P(H=j)} \\
    &= \frac{P(e_1|H=k)P(e_2|H=k)P(H=k)}{\sum\limits_{j=1}^K
      P(e_1|H=j)P(e_2|H=j)P(H=j)} \qquad \text{(iii.)}
\end{align*}

(i.), (ii.), and (iii.) are each sufficient.

\section{Pairwise independence does not imply mutual independence}

Example: Let X and Y represent two coin tosses, with value 0 or 1 for
heads or tails, respectively. Let Z = 1 if two coin tosses result in
exactly one heads.

\[ (X, Y, Z) = \left\{
  \begin{array}{ccc}
    ( 0, & 0, & 0 ) \\
    (0, & 1, & 1) \\
    (1, & 0, & 1) \\ 
    (1, & 1, & 0)
  \end{array}
\right\}
\]

\begin{gather*}
  P(X=1) = P(Y=1) = P(Z=1) = \frac{1}{2} \\
  P(X=1|Y=1) = P(X=1|Z=1) = P(Y=1|Z=1) = \frac{1}{2} \\
  P(X=1,Y=1,Z=1) \stackrel{?}{=} P(X=1)P(Y=1)P(Z=1) \\
  0 \neq \frac{1}{2} \cdot \frac{1}{2} \cdot \frac{1}{2}
\end{gather*}

The state of any two variables $\in (X, Y, Z)$ determines the third.
This means that $X$, $Y$, and $Z$ are pairwise independent, but
\textbf{not} mutually independent.

\section{Conditional independence iff joint factorizes}

\setcounter{equation}{0}

By definition, $X \ci Y | Z$ iff

\begin{equation} \label{eq:8-1}
  p(x,y|z) = p(x|z)p(y|z)
\end{equation}

We wish to prove an alternate definition - $X \ci Y | Z$ iff there
exist functions $g$ and $h$ such that

\begin{equation} \label{eq:8-2}
  p(x,y|z) = g(x,z)h(y,z)
\end{equation}

for all $x, y$ and $z$ such that $p(z) > 0$.

To prove this we will show that given equation \ref{eq:8-2}, equation
\ref{eq:8-1} follows, and $X \ci Y | Z$.

We begin by integrating out $x$.

\begin{align*}
  \int_x p(x,y|z) dx &= \int_x g(x,z)h(y,z)dx \\
  p(y|z) &= h(y,z)g(z) \\
  p(y|z) \frac{1}{g(z)} &= h(y,z)
\end{align*}

By a symmetric argument, we have $p(x|z) \frac{1}{h(z)} = g(x,z)$.
Plugging into our original equation, we have

\begin{align*}
  p(x,y|z) &= p(y|z)p(x|z)\frac{1}{g(z)}\frac{1}{h(z)} \\
  \int_x \int_y p(x,y|z) dx dy &= \int_x \int_y
                                 p(y|z)p(x|z)\frac{1}{g(z)}\frac{1}{h(z)}
  dx\,dy \\
  1 &= \frac{1}{g(z)}\frac{1}{h(z)} \\
  p(x,y|z) &= p(y|z)p(x|z) \cdot 1 \qed
\end{align*}

\section{Conditional independence}

\subsection{}

\setcounter{equation}{0}

\[
\overbrace{(X \ci W | Z,Y)}^{A} \wedge \overbrace{(X \ci Y | Z)}^{B}
\stackrel{?}{\implies} (X \ci Y,W | Z)
\]

By decomposition of $(X \ci Y,W | Z)$, we have

\begin{equation} \label{eq:9-1}
  X \ci Y | Z
\end{equation}

\begin{equation} \label{eq:9-2}
  X \ci W | Z
\end{equation}

(\ref{eq:9-1}) is trivally true by (B). Now we must examine
(\ref{eq:9-2}).

\[
\overbrace{(X \ci W | Z,Y)}^{A} \wedge \overbrace{(X \ci Y | Z)}^{B}
\stackrel{?}{\implies} (X \ci W | Z)
\]

\begin{align*}
  P(X|WYZ) &= P(X|YZ) \quad &\text{by (B)} \\
  &= P(X|Z) \quad &\text{by (A)} \\
  P(X|WYZ) &= P(X|WZ) \quad &\text{by (A)} \\
  P(X|Z) &= P(X|WZ) \implies X \ci W | Z
\end{align*}

Both (\ref{eq:9-1}) and (\ref{eq:9-2}) are true, so it is true that
\[
(X \ci W | Z,Y) \wedge (X \ci Y | Z) \implies (X \ci Y,W | Z)
\]

\subsection{}

\[
(X \ci Y | Z) \wedge (X \ci Y | W) \stackrel{?}{\implies} (X \ci Y | Z,W)
\]

We disprove this with a counter example. Let $X$, $Y$, and $Z$ be
i.i.d., where

\[
P(X) =
\begin{cases}
.5, \quad &\text{ X = 1} \\
.5, \quad &\text{X = -1}
\end{cases}
\]

Let $W = XYZ$.

\par
Given $Z$, knowledge of $Y$ gives no additional information about $X$.
\par
Given $W$, knowledge of $Y$ gives no additional information about $X$.
\par
Given $W,Z$ however, $X$ and $Y$ are \textbf{not} independent, because
$XY = WZ$.

\section{Deriving the inverse gamma density}

\begin{align*}
  X &\sim \text{Ga}(a,b) \\
  p(x) &= \text{Ga}(x|a,b) = \frac{b^a}{\Gamma(a)} x^{a-1}e^{-xb} \\
  P_{x}(x) &= \int{f(x) \; dx} \\
  P_{x}(x) &= \int{\frac{b^{a}}{\Gamma(a)} x^{a-1} e^{-xb} \; dx}
\end{align*}

Now we change variables to find the cdf of $Y$, $P_{y}(y)$
\begin{align*}
  x &= \frac{1}{y} = y^{-1} \\
  \frac{dx}{dy} &= -y^{-2} = -\frac{1}{y^{2}} \\
  dx &= -\frac{1}{y^{2}} dy \\
  P_{y}(y) &= \int{ \frac{b^{a}}{\Gamma(a)}
             \left( y^{-1} \right) ^{a-1} e^{-\frac{b}{y}}
             y^{-2} \; dy} \\
    &= \int{ \frac{b^{a}}{\Gamma(a)}
      y^{-(a+1)} e^{-\frac{b}{y}}} \; dy \\
  p(y) &= \frac{b^{a}}{\Gamma(a)}
      y^{-(a+1)} e^{-\frac{b}{y}} \\
  &= IG(y|a,b)
\end{align*}

\section{Normalization constant for a 1D Gaussian}

After squaring Z and converting to polar coordinates, we have
\begin{align*}
  Z^{2} &= \int_0^{2\pi} \int_0^\infty re^{\frac{-r^{2}}{2\sigma^{2}}}
          \; dr d\theta \\
  &= \Big[ \theta \int_0^\infty{re^{-\frac{r^{2}}{2\sigma^{2}}} \; dr}
    \Big]_0^{2\pi} \\
  &= 2\pi \int_0^\infty{re^{-\frac{r^{2}}{2\sigma^{2}}}} \; dr
\end{align*}

Let $u = \phi(r) = e^{-\frac{r^{2}}{2\sigma^{2}}}$.

\begin{align*}
  \frac{du}{dr} &= -\frac{r}{\sigma^{2}}
                  e^{-\frac{r^{2}}{2\sigma^{2}}} \\
  du &= -\frac{r}{\sigma^{2}}
                  e^{-\frac{r^{2}}{2\sigma^{2}}} dr \\
  \frac{-\sigma^{2}}{u} du &= r \; dr \\
  \phi(0) &= 1 \\
  \phi(\infty) &= e^{-\infty} = 0
\end{align*}

Now we plug $u$ and $r \; dr$ back into our equation for $Z^2$.
\begin{align*}
  Z^2 &= 2\pi \int_1^0{u -\frac{\sigma^{2}}{u}} \; du \\
  &= -2\pi \int_1^0{\sigma^{2}} \; du \\
  &= -2\pi \sigma^{2} \cdot \Big[ u \Big]_1^0 \\
  &= 2\pi \sigma^{2} \\
  Z &= \pm \sqrt{2\pi\sigma} \\
  &= \sqrt{2\pi\sigma}
\end{align*}

We take only the positive value, since we arbitrarily squared Z at the
beginning of our derivation.

\section{Expressing mutual information in terms of entropies}

\begin{align*}
  I \left( X, Y \right) &= KL \left( p \left( X,Y \right) || p \left( X \right) p
  \left( Y \right) \right) \\
  &= \sum_x \sum_y p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \\
  H(X) &= - \sum_{k=1}^K p(X=k) \log p(X=k) \\
  &= - \sum_x p(x) \log p(x) \\
  H(X|Y) &= \sum_y p(y) H(X|Y=y) \\
  &= \sum_y p(y) \left( - \sum_x p(x) \log p(x) | Y=y \right) \\
  &= - \sum_y \sum_x p(x,y) \log \frac{p(x,y)}{p(y)} \\
  H(X) - H(X|Y) &= - \sum_x p(x) \log p(x) + \sum_y \sum_x p(x,y) \log
                  \frac{p(x,y)}{p(y)} \\
  &= - \sum_y{p(y)} \sum_xp(x) \log p(x) + \sum_y \sum_x p(x,y) \log
    \frac{p(x,y)}{p(y)} \\
  &= - \sum_y \sum_x p(x,y) \log p(x) + \sum_y \sum_x p(x,y) \log
    \frac{p(x,y)}{p(y)} \\
  &= \sum_x \sum_y p(x,y) \log \frac{p(x,y)}{p(x)p(y)} \\
  &= I(X,Y) \qed
\end{align*}

\section{Mutual information for correlated normals}

\begin{align*}
  I(X,Y) &= h(X) - h(X|Y) \\
  &= h(X) - \left( h(X,Y) - h(Y) \right) \\
  &= h(X) + h(Y) - h(X,Y) \\
\end{align*}

For a 1-d Gaussian,
\[
h(X) = \frac{1}{2} log_2 \left[ 2 \pi e \sigma^2 \right]
\]

For a 2-d Gaussian,
\[
h(X) = \frac{1}{2} log_2 \left[ \left( 2 \pi e \right)^2 \sigma^4
  \left( 1 - \rho^2 \right) \right]
\]

$X_1$ and $X_2$ are individually normal with variance $\sigma^2$, so
$h(X_1) = h(X_2)$.

\begin{align*}
  I(X_1, X_2) &= h(X_1) + h(X_2) - h(X_1, X_2) \\
  &= \log_2 (2 \pi e)^2 \sigma^4 - \frac{1}{2} \log_2 (2 \pi e)^2
    \sigma^4 (1 - \rho^2) \\
  &= - \frac{1}{2} \log_2 (1 - \rho^2)
\end{align*}

\begin{enumerate}
  \item $\rho = 1; \; I(X_1, X_2) = \infty$ \qquad (perfect correlation,
    $X_1 = X_2$) \\
  \item $\rho = 0; \; I(X_1, X_2) = 0$ \qquad (independent) \\
  \item $\rho = -1; \; I(X_1, X_2) = \infty$ \qquad (perfect correlation,
    $X_1 = -X_2$) \\
\end{enumerate}

\section{A measure of correlation (normalized mutual information)}

\begin{equation*}
  r = 1 - \frac{H(Y|X)}{H(X)}
\end{equation*}

\subsection{}
\begin{align*}
  I(X,Y) &= H(X) - H(Y|X) \\
  r &= \frac{H(X)}{H(X)} - \frac{H(Y|X)}{H(X)} \\
  &=  \frac{H(X)}{H(X)} - \frac{H(X|Y)}{H(X)} \qquad \text{(because }
    H(X) = H(Y)\text{)} \\
  &= \frac{I(X,Y)}{H(X)}
\end{align*}

\subsection{}
$
\begin{array}{rcl}
  0 \leq 
  & H(X|Y)
  & \leq H(X) \\
  0 \leq
  & \frac{H(X|Y)}{H(X)}
  & \leq 1 \\
  0 \leq 
  & \frac{H(Y|X)}{H(X)}
  & \leq 1 \\
  0 \leq
  & 1 - \frac{H(Y|X)}{H(X)}
  & \leq 1 \\
  0 \leq
  & r
  & \leq 1
\end{array}
$

\subsection{}
$r = \frac{I(X,Y)}{H(X)}$, so $r = 0$ when $I(X,Y) = 0$. This occurs
when $X$ and $Y$ are independent.

\subsection{}
$r = 1 - \frac{H(X|Y)}{H(X)}$, so $r = 1$ when $H(X|Y)$ = 0. This occurs
when $X$ and $Y$ are perfectly correlated.

\section{MLE minimizes KL divergence to the empirical distribution}

\[
0 \le KL(p || q) = \int_x p(x) \log \left( \frac{p(x)}{q(x)} \right)
dx
\]

$KL(p||q)$ is non-negative, so it is minimized at 0, or when

\begin{gather*}
  p(x) \log p(x) - p(x) \log q(x) = 0 \\
  p(x) \log p(x) = p(x) \log q(x) \\
  \log p(x) = \log q(x) \\
\end{gather*}

$p(x)$ is the empirical distribution, so we have

\begin{equation*}
  p_{emp}(x) =  q(x; \theta)
\end{equation*}

Therefore, $\theta$ must be the MLE.

\begin{equation*}
  p_{emp}(x) = q(x; \boldsymbol{\hat{\theta}})
\end{equation*}

\section{Mean, mode, variance for the beta distribution}
\setcounter{equation}{0}
\begin{equation}
  \label{eq:16-1}
  \text{Beta}(x|a,b) = \frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1}  
\end{equation}

\begin{equation}
  \label{eq:16-2}
  B(a,b) \defeq \frac{\Gamma(a) \Gamma(b)}{\Gamma(a+b)}
\end{equation}

\begin{align*}
  \E[X] &= \int_0^1 x \cdot \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} \; dx
  \\
       &= \frac{1}{B(a,b)} \int_0^1 x^a (1-x)^{b-1} \; dx \\
       &= \frac{1}{B(a,b)} \cdot B(a+1,b) \\
       &= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \cdot
         \frac{\Gamma(a+1)\Gamma(b)}{\Gamma(a+b+1)} \\
       &= \frac{\Gamma(a+1)\Gamma(a+b)}{\Gamma(a)\Gamma(a+b+1)} \\
       &= \frac{a\Gamma(a) \Gamma(a+b)}{\Gamma(a) (a+b) \Gamma(a+b)} \\
       &= \frac{a}{a+b} \qquad \text{(mean)}
\end{align*}

\begin{align*}
  \E[X^2] &= \int_0^1 \frac{1}{B(a,b)} x^{a-1} (1-x)^{b-1} x^2 \; dx \\
         &= \frac{1}{B(a,b)} \int_0^1 x^{a+1} (1-x)^{b-1} \; dx \\
         &= \frac{1}{B(a,b)} B(a+2,b) \\
         &= \frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)} \cdot
           \frac{\Gamma(a+2)\Gamma(b)}{\Gamma(a+b+2)} \\
         &=
           \frac{\Gamma(a+b)(a+1)\Gamma(a+1)}{\Gamma(a)(a+b+1)\Gamma(a+b+1)}
  \\
         &=
           \frac{\Gamma(a+b)(a+1)a\Gamma(a)}{\Gamma(a)(a+b+1)(a+b)\Gamma(a+b)}
  \\
  &= \frac{a(a+1)}{(a+b)(a+b+1)} \\
\end{align*}

\begin{align*}
  \Var[X] &= \E[X^2] - (\E[X])^2 \\
          &= \frac{(a+1)(a)}{(a+b)(a+b+1)} - \left(\frac{a}{a+b}\right)^2 \\
          &= \frac{(a+1)(a)}{(a+b)(a+b+1)} - \frac{a^2}{(a+b)^2} \\
          &= \frac{(a+b)(a+1)a}{(a+b)^2(a+b+1)} -
            \frac{a^2(a+b+1)}{(a+b)^2(a+b+1)} \\
          &= \frac{a(a+b)(a+1) - a^2(a+b+1)}{(a+b)^2(a+b+1)} \\
          &= \frac{a(a^2 + a + ab + b) - a^3 - ba^2 -
            a^2}{(a+b)^2(a+b+1)} \\
          &= \frac{ab}{(a+b)^2(a+b+1)} \qquad \text{(variance)}
\end{align*}

The mode of the distribution occurs where the pdf is at a maximum. To
maximize the pdf, we must find where $p'(x) = 0$.

\begin{align*}
  p(x) &= \frac{x^{a-1}(1-x)^{b-1}}{B(a,b)} \\
  \od{}{x} p(x) &= \frac{(a-1)x^{a-2}(1-x)^{b-1}}{B(a,b)} -
                 \frac{x^{a-1}(b-1)(1-x)^{b-2}}{B(a,b)} = 0 \\
\end{align*}

We can ignore the denominator.

\begin{gather*}
  (a-1)x^{a-2}(1-x)^{b-1} - x^{a-1}(b-1)(1-x)^{b-2} = 0 \\
  x^{a-2} \cdot \left[ (a-1)(1-x)^{b-1} - x(b-1)(1-x)^{b-2} \right] =
  0 \\
  x^{a-2}(1-x)^{b-2} \left[ (a-1)(1-x) - x(b-1) \right] = 0 \\
  a - ax - 1 + x - bx + x = 0 \\
  -x(a + b - 2) + a - 1 = 0 \\
  x = \frac{a-1}{a+b-2} \quad \text{(mode)}
\end{gather*}

\section{Expected value of the minimum}

Let $T = min(X_1,X_2,...,X_N)$. We are interested in $\E[T]$, for which we
will need the pdf of $t$. Let $N$ represent the number of samples.

\begin{align*}
F(t) &= P(T \le t) \\
     &= \begin{cases}
       0 & \quad \text{if } t < a \\
       1 - P(T > t), & \quad \text{for } a \le t \le b \\
       1 & \quad \text{if } t > b \\
     \end{cases} \\
     &= 1 - P(\forall_iX_i > t) \quad \text{for } a \le t \le b\\
     &= 1 - \prod_i P(X_i > t) \quad \text{for } a \le t \le b
       \quad \text{(because samples are indpendent)}\\
     &= 1 - P(X_1 > t)^N \quad \text{for } a \le t \le b\\
     &= 1 - \left( \frac{b - t}{b - a} \right)^N \quad \text{for } a
       \le t \le b
\end{align*}

Now we take the derivative, $f(t)$, which is the pdf.
\begin{align*}
  f(t) &= \od{}{t} F(t) \\
       &= \od{}{t}\left(1 - \left( \frac{b - t}{b - a} \right)^{N}\right) \\
       &= 0 - \od{}{t} \left( \frac{1}{(b-a)^N} \cdot (b - t)^N \right)\\
       &= \frac{N}{(b-a)^N} \cdot (b - t)^{N-1}
\end{align*}

\begin{align*}
   \E[t] &= \int_a^b t \cdot f(t) \; dt \\
   \E[t] &= \frac{N}{(b-a)^N} \int_a^b (b - t)^{N-1} \cdot t \; dt \\
\end{align*}

We can solve this integral using substitution. Let $u = b - t$. Then
$du = -dt$ and $t = b - u$.

\begin{align*}
  E[t] &= \frac{N}{(b-a)^N} \int_{b-a}^0 (u-b) u^{N-1} \; du \\
       &= \frac{N}{(b-a)^N} \int_{b-a}^0 u^N - bu^{N-1} \; du\\
       &= \frac{N}{(b-a)^N} \left[ \left( \frac{u^{N+1}}{N+1} -
         \frac{bu^N}{N} \right) \right]_{b-a}^0 \\
       &= \frac{N}{(b-a)^N} \frac{b(b-a)^N(N+1) -
         N(b-a)^{N+1}}{N(N+1)} \\
       &= \frac{1}{(b-a)^N} \frac{(b-a)^N \left[ b(N+1) - N(b-a)
         \right]}{N+1} \\
       &= \frac{bN + b -bN + Na}{N+1} \\
       &= \frac{Na + b}{N+1} \\
\end{align*}

For $N = 2, a = 0, \text{ and } b = 1$.

\begin{align*}
         \E[t] &= \frac{(2 \cdot 0) + 1}{2 + 1} \\
               &= \frac{1}{3}
\end{align*}

\end{document}
